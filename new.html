<!DOCTYPE html>
<html>
<head>
  <meta charSet='utf-8' />
  <title>JLUie_MotionAnalysis - 基于机器学习的OWAS动作评价改进</title>
  <link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/4.0.0/normalize.min.css' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,600,300,700' rel='stylesheet' type='text/css'>
  <link href='/JLUie_MotionAnalysis/css/docs.css' rel='stylesheet'>
</head>
<body>
  <div class='menu'>
    <div class='logo'>
      Documentation
    </div>
    <nav class='menu-nav'>
      
        <ul class='nav'>
          <span>介绍</span>
          <ul class='nav'>
            
              <li>
                <a href='/JLUie_MotionAnalysis/what-is-it.html'>这是什么?</a>
              </li>
            
              <li>
                <a href='/JLUie_MotionAnalysis/how-it-works.html'>我们在忙什么？</a>
              </li>
            
              <li>
                <a href='/JLUie_MotionAnalysis/how-it.html'>数据采集，数据库建立，人因分析</a>
              </li>
            
              <li>
                <a href='/JLUie_MotionAnalysis/how.html'>数据的机器学习应用例子（以python为例）</a>
              </li>
            
              <li>
                <a href='/JLUie_MotionAnalysis/ho.html'>网站App创建</a>
              </li>
            
              <li>
                <a href='/JLUie_MotionAnalysis/installation.html'>资源共享和txt使用实例分享（matlab）</a>
              </li>
            
              <li>
                <a href='/JLUie_MotionAnalysis/using.html' class='active'>动作视频预览</a>
              </li>
            
          </ul>
        </ul>
      
        <ul class='nav'>
          <span>项目畅想未来</span>
          <ul class='nav'>
            
              <li>
                <a href='/JLUie_MotionAnalysis/new.html'>基于机器学习的OWAS动作评价</a>
              </li>
            
              <li>
                <a href='/JLUie_MotionAnalysis/fuckable.html'>项目展望</a>
              </li>
            
          </ul>
        </ul>
      
    </nav>
    <a class='footer' href='https://github.com/nodejh/hexo-documentation'>
      Project on github
    </a>
  </div>
  <div class='page'>
    <div class='page-content'>
      <h1>基于机器学习的OWAS动作评价改进</h1>
      <p>项目进展到现在，团队又得到了一些喜人的进展，简要来说：</p>
<blockquote>
<p>（1）加强数据采集，丰富数据训练集，基本完成基础动作特征库PC端和移动端的搭建</p>
<p>（2）设计机器学习算法对工人进行基于OWAS的人因工效学评价，对动作进行风险性评级   </p>
<h3 id="1-基本完成基础动作特征库PC端和移动端的搭建"><a href="#1-基本完成基础动作特征库PC端和移动端的搭建" class="headerlink" title="1.基本完成基础动作特征库PC端和移动端的搭建"></a>1.基本完成基础动作特征库PC端和移动端的搭建</h3></blockquote>
<p>建立了符合国人标准的基础动作数据库，数据兼容性格式支持，采用PHP索引目录列表程序，支持多国语言，可在线浏览文本，图片，视频，音频等文件，方便分享交流，便于用户下载。开启二维码扫码下载功能，以便手机用户下载文件。
</p><p>试运行调试中的下载页面，已通过内网穿透调试，尚未正式开服上线<br>    <img src="http://pd8gnaa72.bkt.clouddn.com/h5ai.png" alt=""><br>    本研究所建立的基础动作特征库中将传感器高度设定为为1.5米或1米，与操作者距离2~3米，操作者面向与传感器摄像头相向，操作者分别顺时针旋转0°、45°、90°、135°、180°、225°、270°、315°，对操作者分别进行动作数据采集，使用深度数据进行骨骼追踪，对人体各重要关节出进行标记，一共标记了21个骨骼点。<br>   本研究所建立的基础动作特征库中将传感器高度设定为为1.5米或1米，与操作者距离2~3米，操作者面向与传感器摄像头相向，操作者分别顺时针旋转0°、45°、90°、135°、180°、225°、270°、315°，对操作者分别进行动作数据采集，使用深度数据进行骨骼追踪，对人体各重要关节出进行标记，一共标记了21个骨骼点。<br><img src="http://pd8gnaa72.bkt.clouddn.com/kinect%E5%9D%90%E6%A0%87%E6%84%8F%E4%B9%89.png" alt=""><br>    传感器的采集频率是每秒钟30帧，视频流中每一帧都可以视作一个静态图像，包含着操作者在某时刻的工作姿态信息。各帧数据如图所示：<br><img src="http://pd8gnaa72.bkt.clouddn.com/163%E5%B8%A7%E9%87%8D%E5%8F%A0.JPG" alt=""><br>基础动作数据库具体分类如图：<br><img src="http://pd8gnaa72.bkt.clouddn.com/%E5%A4%A7%E5%88%9B%E5%8A%A8%E4%BD%9C%E5%88%86%E7%B1%BB.png" alt=""><br>    搭建网页，将所收集数据的骨骼坐标值文本文件、火柴人视频文件、代码(<code>Matlab&amp;Python</code>)存入云空间中，作为共享动作特征库。网页界面如图所示。<br>人因动作分析实验室上线官网：<br><img src="http://pd8gnaa72.bkt.clouddn.com/%E7%BD%91%E9%A1%B5%E9%A1%B5%E9%9D%A2.png" alt=""><br>机器学习方法研究动作分析官网下载页面<br>（欢迎用你的小手轻轻点击屏幕左边选项，在百度脑图外链下载体验）<br><img src="http://pd8gnaa72.bkt.clouddn.com/%E8%B5%84%E6%BA%90%E4%B8%8B%E8%BD%BD%E9%A1%B5%E9%9D%A2.png" alt=""><br>     本研究截止目前，一共采集了211组数据，每组数据包含的训练集有47到573不等的数据单元。此动作库面向从事机器学习算法开发人员、从事大数据分析工作者、从事人因工效学评价研究者。采集并存储了大量置信度较高的底层数据，包括相关动态坐标值、人体骨骼模型仿真图样以及相关可编译程序代码等，为机器学习算法的开发与应用提供数据保障，提供了一种新型的动作分析方法的研究思路。  ——</p>
<h3 id="2-设计机器学习算法对工人进行基于OWAS的人因工效学评价，对动作进行风险性评级"><a href="#2-设计机器学习算法对工人进行基于OWAS的人因工效学评价，对动作进行风险性评级" class="headerlink" title="2.设计机器学习算法对工人进行基于OWAS的人因工效学评价，对动作进行风险性评级"></a>2.设计机器学习算法对工人进行基于OWAS的人因工效学评价，对动作进行风险性评级</h3><p><strong>2.1 OWAS方法介绍</strong><br>    OWAS是一种人体工程学基于观察或调查的常用分析评估工作姿势的方法，常被用来确定肌肉骨骼病的风险并提示改善工作姿势的方案，应用十分广泛。在OWAS 方法中，分别将背部姿势、手臂姿势、腿部姿势以及负荷或使用力量编码，组成一个四位数字编码，如图5所示：<br> SWA 原始编码方式:<br><img src="http://pd8gnaa72.bkt.clouddn.com/OSWA%E5%8E%9F%E5%A7%8B%E7%BC%96%E7%A0%81%E6%96%B9%E5%BC%8F.png" alt=""><br>    与其他工作姿势评价方法相比，OWAS除了对载荷或力量、背部姿势、手臂姿势和腿部姿势进行编码以外，还考虑了姿势的持续时间，可以评估静态姿势，也可以评估动态姿势，工作场所既可以是固定的，也可以是移动的。而通过光学测量传感器，身体部位的空间位置和某种工作姿势的持续时间可以方便准确地获得。因此，OWAS更适合于计算机辅助的工作姿势研究，以便于实现自动化。<br>OWAS方法将身体姿势编码加以统计后，得到可以用来判断操作人员工作姿势行动等级AC。在该方法中，行动等级分为四个等级AC1、AC2、AC3、AC4。如表格1所示：<br><img src="http://pd8gnaa72.bkt.clouddn.com/%E8%A1%8C%E5%8A%A8%E7%AD%89%E7%BA%A7.PNG" alt=""><br>    除了AC1以外，其他四个行动等级均有动作姿势改善或动作进程间断性的需要。其中颈部、腰部、上肢和下肢为基础，决定了基本行动等级，为AC1、AC2、AC3。动作频率或持续时间作为编码辅助，在基本行动等级之上加一到二级。<br>    基于基础特征库中所标记出的人体重要骨骼点的坐标，利用深度学习的方法，搭建多层人工神经网络，以高效简单的方法对操作者的姿势进行OWAS危险等级评估。<br><strong>2.2 改进后的OSWA方法</strong><br>    基于人因动作分析需求，本研究在OWAS的基础上采用了颈部、腰部、上肢、下肢、动作频率或持续时间来对工人的动作进行评价。其中颈部、腰部、上肢、下肢动作为基础特征，动作频率或持续时间为附加特征，如图6所示：<br><img src="http://pd8gnaa72.bkt.clouddn.com/%E6%94%B9%E8%BF%9BOSWA.PNG" alt=""><br>改进后的五维编码评价标准，描述本研究中OWAS方法对于工作姿势的具体编码方式。<br><img src="http://pd8gnaa72.bkt.clouddn.com/%E6%94%B9%E8%BF%9BOWAS%E7%BC%96%E7%A0%81%E6%96%B9%E5%BC%8F.png" alt=""><br>    本研究的OWAS方法将身体姿势编码加以统计后，得到可以用来判断操作人员工作姿势行动等级AC。在该方法中，行动等级分为四个等级AC1、AC2、AC3、AC4。除了AC1以外，其他四个行动等级均有动作姿势改善或动作进程间断性的需要。其中颈部、腰部、上肢和下肢为基础，决定了基本行动等级，为AC1、AC2、AC3。动作频率或持续时间作为编码辅助，在基本行动等级之上加一到二级。<br>    针对动态过程，根据美国职业安全和健康机构（NIOSH）提出的修正后的提举公式中提举重量推荐限值（RWL）和提举指标（LI），本研究在RWL的基础上，对外科手术中医生的类提举动作的工效学负荷分析以及危险性评估中进行了定性地研究和客观定量的评价，通过提举公式计算出动态过程中标准动作频率（lifting/min）的阈值。在高频重复（频率&gt;2 lifting/min）动作中，如果动作频率超过这个频率阈值，AC等级在原有的基本AC等级上增加。评价过程如图8所示：</p>
<p>动态评价模型原理</p>
<p><img src="http://pd8gnaa72.bkt.clouddn.com/%E5%8A%A8%E6%80%81%E8%AF%84%E4%BB%B7%E6%B5%81%E7%A8%8B.png" alt=""></p>
<p>针对静态过程，将其视为低频过程，即频率&lt;2 lifting/min，过长时间保持相同姿势和负载。 本文结合了OWAS方法与ULRA方法（upper limbs risk assessment），将低频静态负载过程持续时间分成三类：(1)持续一小时以下间断性操作；(2)持续时间大于一小时并小于两小时；(3)持续时间大于等于两小时，这三类对于MSDs的风险等级递增。因此在静态与低频重复动作辅助的SP-OWAS中，基本行动等级为AC1的动作在持续时间大于等于1小时而小于2小时时，行动等级增加为AC2；在持续时间大于等于2小时时，行动等级增加为AC3。基本行动等级为AC2的动作，在持续时间大于等于5min，小于15min时，行动等级增加为AC3；在持续时间大于等于15min时，行动等级增加为AC4。基本行动等级为AC3的动作，在持续时间大于等于5min时，行动等级增加为AC4。评价过程如图9所示：
</p><p>静态评价原理</p>
<p><img src="http://pd8gnaa72.bkt.clouddn.com/%E9%9D%99%E6%80%81%E8%AF%84%E4%BB%B7%E6%A8%A1%E5%9E%8B.png" alt=""></p>
<p><strong>2.3 利用深度学习辅助OWAS编码</strong><br>    由深度传感器获取的人体数据为63维，该数据集具有多重共线性与稀疏性，因此首先利用主成分分析（PCA），以减少预测变量个数，确保变量相互独立。PCA通常用于诸如有损数据压缩、数据挖掘和图像处理等。PCA主要需要计算协方差矩阵的特征值和特征向量，通过继算属于协方差矩阵的特征值和相应的特征向量来提供变换矩阵。奇异值分解（SVD）是用于计算特征值和特征向量的有效方法之一，SVD直接应用于数据矩阵，并产生左右奇异值。</p>
<p>                            SVD方法公式：X = USV^T    

其中，U和V是包含左右奇异值的矩阵，这些矩阵是指属于XX^T和X^T X的特征向量。矩阵S是指属于协方差矩阵的特征值的正平方根。属于协方差矩阵的特征值决定了相应特征向量的方差多大程度上是解释性的。PCA旨在通过忽略特征向量来消除噪声，通过将数据仅投影到未忽略的特征向量空间来重新形成数据。
将数据经过PCA分析之后进行特征缩放（feature scaling），来标准化数据特征的范围，并加快下文随机梯度下降时的收敛速度。
因为OWAS在使用过程中，基本行动等级AC1、AC2、AC3相互之间的区别相对较为模糊且为静态过程不包含时间序列，所以在对人体数据完成预处理之后，选择人工神经网络（ANN）的方法来对基本行动等级进行分类。ANN使用前文所说的63维人体骨骼数据经过PCA降维后的数据为基础，来对基本行动等级进行分类。输出集由三个正交向量组成[1,0,0]，[0,1,0]和[0,0,1]，分别代表AC1,AC2,AC3。
 这个ANN的输入层为10个神经元；共含有3个隐藏层，每个隐藏层由7个神经元组成；输出层由3个神经元组成。隐藏层选择线性整流函数（ReLU）为激活函数，输出层的激活函数为softmax，使用随机梯度下降对网络进行训练，然后用k-fold cross validation来对模型进行评估。最后通过训练好的人工神经网络的3×3混淆矩阵对角线元素占百分比的平均值作为OWAS的性能参数。
在确定了基本行动等级之后，计算最终行动等级。基本行动等级没有发生变化的条件下，利用多线程计时器，分别计算当前基础行动等级下的动态过程时间和静态过程持续时间时间，计时器一在动态过程起点开始计时，满足两个条件其中之一时重置：基本AC等级发生变化或持续时间到达一分钟，计算类提举动作的频率。计时器二在当前类提举动作结束开始计时，满足两个条件其中之一时重置：基本AC等级发生变化或出现下一个类提举动作。
<p><p><strong>2.4 人为分类动作实验</strong><br>    在实验室条件下，设定传感器高度为1.5米，与操作者距离2~3米，操作者面向与传感器摄像头相向，操作者分别顺时针旋转0°、45°、90°、135°、180°、225°、270°、315°，抽取实验样本，计算机分类与人工分析结果不同的帧，由两位成员共同讨论分析并做出相应妥协，结果绘制混淆矩阵如表格：<br><img src="http://pd8gnaa72.bkt.clouddn.com/%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5.PNG" alt=""><br>    观察混淆矩阵可知，人为分类实验准确率为98.93%，其中伪阳性与伪阴性错误分布合理，证明该方法及模型选择合理，效果符合预期。<br><strong>2.5 模型试验</strong><br>    本试验针对基础特征库中采集的动作数据。首先，训练分类AC1、AC2、AC3风险等级的模型，然后调取基础特征库中“拄拐走1.0B.csv”文件。程序如图10所示：<br>   本试验针对基础特征库中采集的动作数据。首先，训练分类AC1、AC2、AC3风险等级的模型，然后调取基础特征库中“拄拐走1.0B.csv”文件。如图所示：<br><img src="http://pd8gnaa72.bkt.clouddn.com/%E4%BD%BF%E7%94%A8%E8%AE%AD%E7%BB%83%E5%87%BA%E6%9D%A5%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E9%A2%84%E6%B5%8B.jpg" alt=""><br>    OWAS方法分类结果如图11所示，第一列即0代表AC1，第二列即1代表AC2，第三列即2代表AC3。<br><img src="http://pd8gnaa72.bkt.clouddn.com/%E7%9F%A9%E9%98%B5%E6%A8%A1%E5%9E%8B%E5%88%86%E7%B1%BB.png" alt=""><br>根据图11的分类结果，确定动作拄拐走是一个行动等级为AC3的动作，该姿势明显有害，需要尽快改进。</p>
<h3 id="3-项目进度记录"><a href="#3-项目进度记录" class="headerlink" title="3.项目进度记录"></a>3.项目进度记录</h3><p><strong>3.1 阶段成果：</strong></p>
<ul>
<li>一项计算机软件著作权（基于体感技术的实时动作检测软件）<pre><code>3项发明专利受理
</code></pre>  一种虚拟装配操作中对准动作识别方法：申请号：201810467080.X<br>  一种弯曲动作识别方法：申请号：201810722880.1<br>  一种虚拟装配坐式操作中目视动作识别方法：申请号：201810256008.2</li>
<li>一篇高水平论文： Artificial Neural Network -aided Surgical procedure working posture analysis :Surgical-Proprietary-Ovako Working Posture Analysis System(SP-OWAS)<br><strong>3.2 存在的问题及拟采取的措施：</strong><br>1.Kinect自身识别导致骨骼点读取不清通过调整设备距离或角度后重新启动解决；<br>2.红外线深度传感器固有的弊端，在场景对人体遮挡面积过大时会影响骨骼点位置坐标的采集和预测。在今后拟利用大量数据和数据挖掘对被遮挡骨骼点进行更准确的预测以消除这一弊端，保持识别准确率不降低。<br>3.在今后的使用过程中可以进一步收集大量数据对本方法继续优化调整，指定标准化，进一步推广为行业标准。针对领域特异性稍作部分参数改动便可以同样对降低农业、工业、建筑业等行业工人患MSDs的几率做出贡献。<br><strong>3.3 下一阶段工作计划：</strong><br>将OWAS动作分析系统整体程序化，以用户软件的形式呈现。<br>继续完善基础动作特征库。<br>除了用OWAS方法对人体动作进行分析评价以外，拟采用其他方法，例如对姿势的模特值进行计算、RULA评价（快速上肢评估）、计算各关节角度等。<br>指导教师意见：<br>  项目组成员定期讨论项目进展，按预期计划合理分配任务。立足于机器学习算法，进一步完善了标准动作数据库，提出的改进后的OWAS方法，可对工人的工作过程进行实时监测，能有效降低工人罹患MSDs的风险，侧面对提高工作效率也有一定的帮助。方法实现了工作姿态的计算机辅助人体工程学风险评价，与传统的人工评价方法相比，更加高效、方便，占用计算机资源少，普适性好，识别准确率高，实现了高度自动化与智能化。不足之处是基础动作特征库的动作样本略少，希望在下阶段的工作中进一步收集大量数据对本方法继续优化调整，进一步推广到其他行业中。</li>
</ul>

    </div>
  </div>
  <div class='switch-page'>
    
      <a class='previous' href='/JLUie_MotionAnalysis/using.html'>Previous</a>
    
    
      <a class='next' href='/JLUie_MotionAnalysis/fuckable.html'>Next</a>
    
  </div>
</body>
</html>
